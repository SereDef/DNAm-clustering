use_library = '/home/s.defina/R/x86_64-pc-linux-gnu-library/4.4'
.libPaths(use_library)

dataset <- c("GENR", "ALSP")
array <- c("450K", "EPIC")

timepoint <- "birth"
normalization <- "funcnorm"

input_dir <- '~/MPSR/data'
methyldata_dir <- file.path(input_dir, 'raw')
metadata_dir <- file.path(methyldata_dir, 'plate_info')

output_dir <- "~/MPSR/data/clean"
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

# TODO: parallelize if necessary (currently done sequentially)

# Construct arguments grid -----------------------------------------------------
args <- expand.grid(dataset = dataset, array = array, timepoint = timepoint, 
                    normalization = normalization)
args <- args[!(args$dataset == "ALSP" & args$array == "EPIC"), ]  # No EPIC data in ALSPAC

rm(use_library, dataset, array, timepoint, normalization)

# Prepare the merged batch correction file -------------------------------------
plates_files <- file.path(metadata_dir, paste(args, "plates.rds", sep = "_"))

plate_info_list <- do.call(mapply, c(function(dataset, array, timepoint, normalization) {
  plate_file <- file.path(metadata_dir, paste(dataset, array, timepoint, 
                                              "plates.rds", sep = "_"))
  plate_info <- readRDS(plate_file)
  
  plate_info$Dataset <- dataset
  plate_info$Array <- array
  
  # Collapse any plates with less than 5 people on them 
  plate_info$Plate <- forcats::fct_lump_min(plate_info$Sample_Plate, min = 5, 
                                            other_level = "other")
  # Rename plates so that they are sample specific
  plate_info$Plate <- as.factor(paste0(dataset, array,"_", plate_info$Plate))
  
  plate_info$Sample_Plate <- NULL
  
  return(plate_info)
  }, args, SIMPLIFY = FALSE))

plate_info <- do.call(rbind, plate_info_list)
rm(plate_info_list)

# Check whether IDs need to be recoded
# any(duplicated(plate_info$Sample_ID)) # FALSE
# Check "new" plate distribution
table(plate_info$Plate)

# Prepare for merging: get the total size of the matrix and the row counts for
# each of the 3 dataset
counts <- as.data.frame(table(plate_info$Dataset, plate_info$Array))
names(counts) <- c('dataset','array', 'count')

args$count <- counts[!(counts$dataset == "ALSP" & counts$array == "EPIC"), "count"]
args$start <- c(0, cumsum(args$count)[-nrow(args)])+1
args$end <- cumsum(args$count)

rm(counts)
save(plate_info, args, file=file.path(output_dir, 'mega_info.RData'))

# ==============================================================================
# STACK ALL DATASETS
# ==============================================================================

# "High-confidence" CpG list generated by 0.2.cpg_selection.R ------------------
cpgs_to_keep <- scan(file.path(input_dir, 'clean_cpg_list.txt'), 
                     what = character(), quiet = TRUE)

message(' * Retaining: ', length(cpgs_to_keep), ' CpGs.')

# Create output mega big.matrix with all the data in it
n_probes <- length(cpgs_to_keep)
n_samples <- nrow(plate_info) # everything 

# Initialize mega matrix
mega_desc <- file.path(output_dir, "mega_filtered.desc")

if (file.exists(mega_desc)) {
  warning(' * "', mega_desc, '" already exists, removing it.')
  file.remove(mega_desc)
  file.remove(gsub('.desc', '.bin', mega_desc))
}

message('Generate (mega) output methylation matrix...')
mega_x <- bigmemory::filebacked.big.matrix(
  nrow = n_probes,
  ncol = n_samples,
  type = "double",
  backingpath = output_dir,
  descriptorfile = basename(mega_desc),
  backingfile = gsub('.desc', '.bin', basename(mega_desc)),
  dimnames = list(cpgs_to_keep, plate_info$Sample_ID)
)

# Set up parallel backend
#future::plan(future::multisession, workers = 3)
#doFuture::registerDoFuture()

# Merge (in parallel)
foreach::foreach(i = 1:nrow(args),
                 .packages = c("bigmemory"),
                 .export = c("args", "methyldata_dir", "mega_desc",
                             "cpgs_to_keep", "plate_info"),
                 .combine = 'c') %do% { # %dorng% {
                   
  dataset <- args[i, 'dataset']
  array <- args[i, 'array']
  timepoint <- args[i, 'timepoint']
  normalization <- args[i, 'normalization']
  
  sample_size <- args[i, 'count']
  col_start <- args[i, 'start']
  col_end <- args[i, 'end']
  
  message("\n=====================================================================")
  message("Dataset: ", dataset, 
          "\nArray: ", array, 
          "\nTime point: ", timepoint, 
          "\nNormalization: ", normalization, 
          "\nSample size: ", sample_size)
  message("=====================================================================")
  
  methyldata_filename <- paste(dataset, array, timepoint, normalization, sep = "_")
  
  message('\nLoading methylation data...')
  x <- bigmemory::attach.big.matrix(file.path(methyldata_dir, 
                                              paste0(methyldata_filename, '.desc')),
                                    readonly = TRUE, lockfile = TRUE)
  message(' * Original dimentions: ', dim(x)[1], ' x ', dim(x)[2])
  
  # Make sure all CpGs are present
  stopifnot(all(cpgs_to_keep %in% rownames(x)))
  # Make sure ID order matches
  plate_info_ids <- plate_info[col_start:col_end, 'Sample_ID']
  stopifnot(all(colnames(x) == plate_info_ids))
  # stopifnot(all(colnames(x) %in% plate_info$Sample_ID))
  
  # Reattach to ensure lock *should not be a problem unless in parallel but 
  # just in case I need to parallelize this in the future
  mega_x <- bigmemory::attach.big.matrix(mega_desc, lockfile = TRUE)
  
  message('Writing mega matrix...')
  mega_x[, col_start:col_end] <- x[cpgs_to_keep, ]
  
  invisible(NULL)
}
