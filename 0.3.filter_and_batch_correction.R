dataset <- "GENR"
array <- "450K" # "EPIC", "450K"
timepoint <- "birth"
normalization <- "funcnorm"

# Install if needed
# install.packages(c("bigmemory", "foreach", "future", "doFuture", "progressr"))
# BiocManager::install("sva")

library(foreach)

# =================================== CONFIG ===================================
use_library = '/home/s.defina/R/x86_64-pc-linux-gnu-library/4.4'
.libPaths(use_library)

input_dir <- '~/MPSR/data'
output_dir <- "~/MPSR/data"

methyl_file <- paste(dataset, array, timepoint, normalization, sep = "_")
plates_file <- paste(dataset, array, timepoint, "plates", sep = "_")

n_cores <- as.numeric(Sys.getenv("SLURM_CPUS_PER_TASK", unset = "1"))
chunk_size <- 1000 # ceiling(n_probes / n_cores) / 2

# ==============================================================================

message('Loading methylation data...')
x <- bigmemory::attach.big.matrix(file.path(input_dir,
                                            paste0(methyl_file, ".desc")))
message(' * Original dimentions:')
dim(x)

# "High-confidence" CpG list generated by 0.2.cpg_selection.R ------------------
cpgs_to_keep <- scan(file.path(input_dir, 'clean_cpg_list.txt'), 
                     what=character(), quiet = TRUE)
stopifnot(all(cpgs_to_keep %in% rownames(x)))

message(' * Retaining: ', length(cpgs_to_keep), ' CpGs.')

# Batch effects information (i.e. plate) ---------------------------------------
plate_info <- readRDS(file.path(input_dir, paste0(plates_file,'.rds')))

# Make sure IDs are in the same order 
plate_info <- plate_info[match(colnames(x), plate_info$Sample_ID), ]
stopifnot(all(colnames(x) == plate_info$Sample_ID))

# x <- x[, plate_info$Sample_ID]
# dim(x)

batch <- plate_info$Sample_Plate

message(' * Plates:')
table(batch)

# ==============================================================================
# BATCH CORRECTION
# ==============================================================================

# This does not account for non-normality and only fixed effects of plates 
# library(limma)
# x_corrected <- limma::removeBatchEffect(x, batch = batch)

# Save descriptors to pass them to parallel workers
x_desc <- file.path(input_dir, paste0(methyl_file, ".desc"))
corrected_desc <- file.path(output_dir, paste0(methyl_file, "_clean.desc"))

# Create output big.matrix
n_probes <- length(cpgs_to_keep)
n_samples <- ncol(x)

# Clean up previous matrices if necessary
if (file.exists(corrected_desc)) {
  warning(' * "', corrected_desc, '" already exists, removing it.')
  file.remove(corrected_desc)
  file.remove(gsub('.desc', '.bin', corrected_desc))
}

message('Generate output methylation matrix...')
corrected_x <- bigmemory::filebacked.big.matrix(
  nrow = n_probes,
  ncol = n_samples,
  type = "double",
  backingpath = output_dir,
  descriptorfile = paste0(methyl_file, "_clean.desc"),
  backingfile = paste0(methyl_file, "_clean.bin"),
  dimnames = list(cpgs_to_keep, colnames(x))
)

# Chunks of CpGs to process in parallel
chunk_seq <- split(cpgs_to_keep,
                   ceiling(seq_along(cpgs_to_keep) / chunk_size))

# Parallel-safe reproducibility
RNGkind("L'Ecuyer-CMRG")
set.seed(123)

# Parallel-safe progress updates
progressr::handlers(global = TRUE)
# progressr::handlers("txtprogressbar")
progressr::handlers(list(
  bar = progressr::handler_progress(
    format   = ":current/:total [:bar] :percent in :elapsed",
    width    = 60, complete = "="),
  file = progressr::handler_filesize(
    file = "slurm.progress",
    intrusiveness = 0)))

message('Running analysis...')
# Set up parallel backend
future::plan(future::multisession, workers = n_cores)
doFuture::registerDoFuture()

progressr::with_progress({
  p <- progressr::progressor(steps = length(chunk_seq))
  
  # Run ComBat in parallel
  foreach(chunk = chunk_seq,
          .packages = c("bigmemory", "sva"),
          .export = c("batch", "x_desc", "corrected_desc")) %dopar% {
            
          # Note: with future framework I need to re-attach matrices
          # this adds a little overhead but ensures the memory pointers are not
          # corrupted -> recommended pipeline on SLURM clusters 
          x <- attach.big.matrix(x_desc, readonly = TRUE, lockfile = TRUE)
          corrected_x <- attach.big.matrix(corrected_desc, lockfile = TRUE)
          
          adjusted_chunk <- NULL
          
          tryCatch({
            data_chunk <- x[chunk, ]
          
            # Run ComBat (non-parametric to avoid normality assumption)
            adjusted_chunk <- suppressMessages(sva::ComBat(dat = data_chunk,
                                               batch = batch, 
                                               par.prior = FALSE))
            
            
          }, error = function(e) {
            message("Chunk failed: ", conditionMessage(e))
          })
          
          if (!is.null(adjusted_chunk)) {
            # Write corrected values
            corrected_x[chunk, ] <- adjusted_chunk
          }
          
          p(class = "sticky")  # Advance progress bar
    }
})
flush.console()

# checking ---------------------------------------------------------------------
# I run this interactively 
# x <- attach.big.matrix(x_desc, lockfile = TRUE)
# corrected_x <- attach.big.matrix(corrected_desc, lockfile = TRUE)
# 
# dim(x)
# dim(corrected_x)
# 
# random_cpgs <- sample(rownames(corrected_x), 30)
# 
# x_subset <- x[random_cpgs, ]
# corrected_x_subset <- corrected_x[random_cpgs, ]
# 
# # Compute correlations row-wise
# cors <- mapply(function(orig, corr) cor(orig, corr),
#                split(x_subset, row(x_subset)),
#                split(corrected_x_subset, row(corrected_x_subset)))
# 
# # Name and print results nicely
# names(cors) <- random_cpgs
# print(cors)

